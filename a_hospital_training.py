"""
Aç—…é™¢ç”¨ å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Automatically converted from Jupyter Notebook for reproducibility.
"""

# ==== Cell 0 ====
# ðŸ¥ Aç—…é™¢ æ—¥æ¬¡äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (é‡è¤‡é™¤åŽ»ãƒ»æ§‹æ–‡ä¿®æ­£ç‰ˆ)
# =================================================================
# 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# =================================================================
!pip -q install japanize-matplotlib jpholiday scikit-learn xgboost shap

# =================================================================
# 2. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# =================================================================
import pandas as pd
import numpy as np
import jpholiday
from datetime import date, timedelta
import io
import json
import joblib
import xgboost as xgb
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from google.colab import files
import warnings
warnings.filterwarnings('ignore')
import shap
import matplotlib.pyplot as plt
import japanize_matplotlib

# =================================================================
# 3. Aç—…é™¢ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# =================================================================
print("--- ðŸ¥ Aç—…é™¢ã®å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ ---")
print("1. æŽ¡è¡€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«")
uploaded_log = files.upload()
print("\n2. å¤–æ¥æ‚£è€…æ•°ãƒ•ã‚¡ã‚¤ãƒ«")
uploaded_patients = files.upload()
print("\n3. æ°—è±¡ãƒ‡ãƒ¼ã‚¿")
uploaded_weather = files.upload()

# =================================================================
# 4. ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# =================================================================
try:
    # --- 4a. ãƒ‡ãƒ¼ã‚¿å‡¦ç† ---
    print("\n--- ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ ---")
    start_dt, end_dt = date(2024, 1, 1), date(2026, 12, 31)
    all_dates_list = [start_dt + timedelta(days=d) for d in range((end_dt - start_dt).days + 1)]
    calendar_features = pd.DataFrame({'date': pd.to_datetime(all_dates_list)})
    is_holiday_series = calendar_features['date'].apply(
        lambda x: jpholiday.is_holiday(x) or x.weekday() >= 5 or \
                    (x.month == 12 and x.day >= 29) or (x.month == 1 and x.day <= 3)
    )
    calendar_features['is_holiday'] = is_holiday_series
    calendar_features['æœˆ'] = calendar_features['date'].dt.month
    weekday_map = {0: 'æœˆ', 1: 'ç«', 2: 'æ°´', 3: 'æœ¨', 4: 'é‡‘', 5: 'åœŸ', 6: 'æ—¥'}
    calendar_features['æ›œæ—¥'] = calendar_features['date'].dt.dayofweek.map(weekday_map)
    calendar_features['é€±å›žæ•°'] = (calendar_features['date'].dt.day - 1) // 7 + 1
    calendar_features['å‰æ—¥ç¥æ—¥ãƒ•ãƒ©ã‚°'] = calendar_features['is_holiday'].shift(1).fillna(False)

    log_df = pd.read_csv(io.BytesIO(next(iter(uploaded_log.values()))), encoding='shift-jis')

    log_df = log_df[log_df['å‡¦ç†'] == 'çµ‚äº†'].copy()
    print(f"âœ… ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œçµ‚äº†ã€ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã—ãŸã€‚å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(log_df)}")

    log_df['å®Ÿæ–½æ—¥'] = pd.to_datetime(log_df['å®Ÿæ–½æ—¥'], format='%y/%m/%d', errors='coerce')
    daily_blood_patients = log_df.groupby('å®Ÿæ–½æ—¥').size().reset_index(name='blood_patient_count')

    total_patients_df = pd.read_csv(io.BytesIO(next(iter(uploaded_patients.values()))), encoding='utf-8-sig', thousands=',')
    total_patients_df['date'] = pd.to_datetime(total_patients_df['date'], errors='coerce')

    weather_raw_df = pd.read_csv(io.BytesIO(next(iter(uploaded_weather.values()))), encoding='shift-jis', header=None, skiprows=3)
    header = weather_raw_df.iloc[0]
    weather_df = weather_raw_df.iloc[3:].reset_index(drop=True)
    weather_df.columns = header

    df_selected = weather_df.iloc[:, [0, 1, 5, 8, 11, 14, 20, 23]].copy()
    df_selected.columns = ['date', 'é™æ°´é‡', 'å¤©æ°—æ¦‚æ³', 'å¹³å‡æ°—æ¸©', 'æœ€é«˜æ°—æ¸©', 'æœ€ä½Žæ°—æ¸©', 'å¹³å‡æ¹¿åº¦', 'å¹³å‡é¢¨é€Ÿ']

    df_selected['é›¨ãƒ•ãƒ©ã‚°'] = df_selected['å¤©æ°—æ¦‚æ³'].str.contains('é›¨', na=False).astype(int)
    df_selected['é›ªãƒ•ãƒ©ã‚°'] = df_selected['å¤©æ°—æ¦‚æ³'].str.contains('é›ª', na=False).astype(int)
    weather_features = df_selected.drop(columns=['å¤©æ°—æ¦‚æ³'])

    weather_features['date'] = pd.to_datetime(weather_features['date'], errors='coerce')
    numeric_cols = ['é™æ°´é‡', 'å¹³å‡æ°—æ¸©', 'æœ€é«˜æ°—æ¸©', 'æœ€ä½Žæ°—æ¸©', 'å¹³å‡æ¹¿åº¦', 'å¹³å‡é¢¨é€Ÿ']
    for col in numeric_cols:
        weather_features[col] = pd.to_numeric(weather_features[col], errors='coerce')
        weather_features[col].fillna(weather_features[col].mean(), inplace=True)

    df = pd.merge(calendar_features, total_patients_df, on='date', how='left')
    df = pd.merge(df, daily_blood_patients, left_on='date', right_on='å®Ÿæ–½æ—¥', how='left')
    df = pd.merge(df, weather_features, on='date', how='left')
    df.dropna(subset=['blood_patient_count', 'total_outpatient_count'], inplace=True)

    # --- 4b. å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åˆ†å‰² ---
    df_encoded = pd.get_dummies(df.drop(columns=['is_holiday', 'å®Ÿæ–½æ—¥'], errors='ignore'), columns=['æ›œæ—¥'], drop_first=True)
    df_encoded['å‰æ—¥ç¥æ—¥ãƒ•ãƒ©ã‚°'] = df_encoded['å‰æ—¥ç¥æ—¥ãƒ•ãƒ©ã‚°'].astype(int)
    train_df = df_encoded[df_encoded['date'].dt.year == 2024].copy()
    val_df = df_encoded[(df_encoded['date'].dt.year == 2025) & (df_encoded['date'].dt.month <= 8)].copy()

    features = [col for col in train_df.columns if col not in ['date', 'blood_patient_count']]
    X_train, y_train = train_df[features], train_df['blood_patient_count']
    X_val, y_val = val_df[features], val_df['blood_patient_count']
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ä»¶, æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val)}ä»¶")

    # --- 4c. XGBoostãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨æœ€é©åŒ– ---
    print("\n--- XGBoostãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã™ (10-fold CV) ---")
    param_grid = {'n_estimators': [100, 200], 'max_depth': [3, 5], 'learning_rate': [0.1, 0.05]}
    grid_search = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror', random_state=42),
                               param_grid=param_grid, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_xgb_model = grid_search.best_estimator_
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†ã€‚æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}")

    # --- 4d. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è©•ä¾¡ ---
    print("\n--- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã™ ---")
    y_pred = best_xgb_model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, y_pred))
    r2 = r2_score(y_val, y_pred)
    print(f"ðŸ“ˆ è©•ä¾¡æŒ‡æ¨™: RMSE={rmse:.2f}, R2ã‚¹ã‚³ã‚¢={r2:.3f}")

    # --- 4e. SHAPã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã®å¯è¦–åŒ– ---
    print("\n--- SHAPã§ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆã‚’å¯è¦–åŒ–ã—ã¾ã™ ---")
    explainer = shap.TreeExplainer(best_xgb_model)
    shap_values = explainer.shap_values(X_val)
    shap.summary_plot(shap_values, X_val, plot_type="bar", max_display=15, show=False)
    plt.title('SHAP Feature Importance (Daily Model)')
    plt.tight_layout()
    plt.show()

    # --- 4f. æˆæžœç‰©ã®ä¿å­˜ ---
    best_params = grid_search.best_params_
    scaler = StandardScaler().fit(X_train)

    joblib.dump(scaler, 'model_A_daily_scaler.joblib')
    best_xgb_model.save_model('model_A_daily.json')
    # â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€ â˜…â˜…â˜…
    with open('best_params_A_daily.json', 'w') as f:
        json.dump(best_params, f)

    print("\nâœ… Aç—…é™¢ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’3ã¤ä¿å­˜ã—ã¾ã—ãŸã€‚")
    print("   - model_A_daily.json (Bç—…é™¢ã¸æä¾›)")
    print("   - best_params_A_daily.json (Bç—…é™¢ã¸æä¾›)")
    print("   - model_A_daily_scaler.joblib (UIã§ä½¿ç”¨)")

    files.download('model_A_daily.json')
    files.download('best_params_A_daily.json')
    files.download('model_A_daily_scaler.joblib')

except Exception as e:
    print(f"\näºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


# ==== Cell 1 ====
# ðŸ¥ Aç—…é™¢ æ™‚é–“å¸¯åˆ¥äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (é‡è¤‡é™¤åŽ»ãƒ»å®‰å®šç‰ˆ)
# =================================================================
# 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# =================================================================
!pip -q install japanize-matplotlib jpholiday scikit-learn xgboost shap

# =================================================================
# 2. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# =================================================================
import pandas as pd
import numpy as np
import jpholiday
from datetime import date, timedelta
import io
import json
import shap
import joblib
import xgboost as xgb
import matplotlib.pyplot as plt
import japanize_matplotlib

from sklearn.metrics import mean_squared_error, r2_score
from google.colab import files
import warnings
warnings.filterwarnings('ignore')

# =================================================================
# 3. Aç—…é™¢ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# =================================================================
print("--- ðŸ¥ Aç—…é™¢ã®å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ ---")
print("1. æŽ¡è¡€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ« (ã€Œå—ä¿¡ã€ã€Œå‡¦ç†ã€åˆ—ã‚’å«ã‚€)")
uploaded_log = files.upload()
print("\n2. å¤–æ¥æ‚£è€…æ•°ãƒ•ã‚¡ã‚¤ãƒ«")
uploaded_patients = files.upload()
print("\n3. æ°—è±¡ãƒ‡ãƒ¼ã‚¿")
uploaded_weather = files.upload()

# =================================================================
# 4. ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# =================================================================
try:
    # --- 4a. å…¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
    print("\n--- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åˆæœŸå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ ---")
    log_df = pd.read_csv(io.BytesIO(next(iter(uploaded_log.values()))), encoding='shift-jis')
    total_patients_df = pd.read_csv(io.BytesIO(next(iter(uploaded_patients.values()))), encoding='utf-8-sig', thousands=',')
    weather_raw_df = pd.read_csv(io.BytesIO(next(iter(uploaded_weather.values()))), encoding='shift-jis', header=None, skiprows=3)

    # ã€Œå‡¦ç†ã€åˆ—ãŒã€Œçµ‚äº†ã€ã®è¡Œã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    log_df = log_df[log_df['å‡¦ç†'] == 'çµ‚äº†'].copy()
    print(f"âœ… ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œçµ‚äº†ã€ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã—ãŸã€‚å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(log_df)}")

    # --- 4b. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ ---
    print("\n--- ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’30åˆ†å˜ä½ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã—ã¾ã™ ---")
    log_df['time'] = pd.to_datetime(log_df['å—ä¿¡'], errors='coerce')
    log_df.dropna(subset=['time'], inplace=True)
    log_df.set_index('time', inplace=True)
    patient_count_by_slot = log_df.resample('30T').size().rename('patient_count_slot')
    patient_count_by_slot = patient_count_by_slot.between_time('08:00', '18:00')
    df_ts = patient_count_by_slot.to_frame()

    # --- 4c. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ---
    print("\n--- æ™‚é–“å¸¯äºˆæ¸¬ç”¨ã®ç‰¹å¾´é‡ã‚’ä½œæˆã—ã¾ã™ ---")

    # æ™‚é–“ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡
    df_ts['hour'] = df_ts.index.hour
    df_ts['minute'] = df_ts.index.minute
    df_ts['dayofweek'] = df_ts.index.dayofweek
    df_ts['date'] = pd.to_datetime(df_ts.index.date)

    # ã€Œä¸€æ—¥ã®å§‹ã¾ã‚Šã€ãƒ•ãƒ©ã‚°
    df_ts['is_first_slot'] = ((df_ts['hour'] == 8) & (df_ts['minute'] == 0)).astype(int)
    df_ts['is_second_slot'] = ((df_ts['hour'] == 8) & (df_ts['minute'] == 30)).astype(int)

    # æ—¥æ¬¡ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ç‰¹å¾´é‡
    daily_dates = pd.DataFrame({'date': pd.to_datetime(df_ts['date'].unique())})
    daily_dates['is_holiday_daily'] = daily_dates['date'].apply(
        lambda x: jpholiday.is_holiday(x) or x.weekday() >= 5 or \
                    (x.month == 12 and x.day >= 29) or (x.month == 1 and x.day <= 3)
    )
    daily_dates['æœˆ'] = daily_dates['date'].dt.month
    daily_dates['é€±å›žæ•°'] = (daily_dates['date'].dt.day - 1) // 7 + 1
    daily_dates['å‰æ—¥ç¥æ—¥ãƒ•ãƒ©ã‚°'] = daily_dates['is_holiday_daily'].shift(1).fillna(False).astype(int)

    # å¤©æ°—ã¨å¤–æ¥æ‚£è€…æ•°ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨ãƒžãƒ¼ã‚¸
    total_patients_df['date'] = pd.to_datetime(total_patients_df['date'], errors='coerce')
    header_w = weather_raw_df.iloc[0]
    weather_df = weather_raw_df.iloc[3:].reset_index(drop=True); weather_df.columns = header_w
    weather_features = weather_df.iloc[:, [0, 1, 5, 8, 11, 14, 20, 23]].copy()
    weather_features.columns = ['date', 'é™æ°´é‡', 'å¤©æ°—æ¦‚æ³', 'å¹³å‡æ°—æ¸©', 'æœ€é«˜æ°—æ¸©', 'æœ€ä½Žæ°—æ¸©', 'å¹³å‡æ¹¿åº¦', 'å¹³å‡é¢¨é€Ÿ']
    weather_features['date'] = pd.to_datetime(weather_features['date'], errors='coerce')
    numeric_cols = ['é™æ°´é‡', 'å¹³å‡æ°—æ¸©', 'æœ€é«˜æ°—æ¸©', 'æœ€ä½Žæ°—æ¸©', 'å¹³å‡æ¹¿åº¦', 'å¹³å‡é¢¨é€Ÿ']
    for col in numeric_cols:
        weather_features[col] = pd.to_numeric(weather_features[col], errors='coerce')
        weather_features[col].fillna(weather_features[col].mean(), inplace=True)
    weather_features['é›¨ãƒ•ãƒ©ã‚°'] = weather_features['å¤©æ°—æ¦‚æ³'].str.contains('é›¨', na=False).astype(int)
    weather_features['é›ªãƒ•ãƒ©ã‚°'] = weather_features['å¤©æ°—æ¦‚æ³'].str.contains('é›ª', na=False).astype(int)
    weather_features['å¤©æ°—ã‚«ãƒ†ã‚´ãƒª'] = weather_features['å¤©æ°—æ¦‚æ³'].str[0]

    df_ts = pd.merge(df_ts, daily_dates, on='date', how='left')
    df_ts = pd.merge(df_ts, total_patients_df, on='date', how='left')
    df_ts = pd.merge(df_ts, weather_features.drop(columns=['å¤©æ°—æ¦‚æ³']), on='date', how='left')

    df_ts = pd.get_dummies(df_ts, columns=['dayofweek', 'å¤©æ°—ã‚«ãƒ†ã‚´ãƒª'], drop_first=True)

    # ãƒ©ã‚°ç‰¹å¾´é‡
    df_ts['lag_30min'] = df_ts['patient_count_slot'].shift(1)
    df_ts['lag_60min'] = df_ts['patient_count_slot'].shift(2)
    df_ts['lag_90min'] = df_ts['patient_count_slot'].shift(3)
    df_ts['rolling_mean_60min'] = df_ts['patient_count_slot'].shift(1).rolling(window=2).mean()

    df_ts.rename(columns={'is_holiday_daily': 'is_holiday'}, inplace=True)
    df_ts.fillna(0, inplace=True)

    # --- 4d. å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åˆ†å‰² ---
    train_df = df_ts[df_ts['date'].dt.year == 2024].copy()
    val_df = df_ts[(df_ts['date'].dt.year == 2025) & (df_ts['date'].dt.month <= 8)].copy()
    train_df.drop(columns=['date'], inplace=True)
    val_df.drop(columns=['date'], inplace=True)

    features = [col for col in train_df.columns if col != 'patient_count_slot']
    X_train, y_train = train_df[features], train_df['patient_count_slot']
    X_val, y_val = val_df[features], val_df['patient_count_slot']
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ä»¶, æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val)}ä»¶")

    # --- 4e. XGBoostãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ ---
    print("\n--- XGBoostæ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¾ã™ ---")
    model = xgb.XGBRegressor(
        objective='reg:squarederror', n_estimators=1000, learning_rate=0.01,
        max_depth=6, early_stopping_rounds=50, random_state=42
    )
    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
    print("âœ… ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†ã€‚")

    # --- 4f. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®è©•ä¾¡ ---
    print("\n--- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã™ ---")
    y_pred = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, y_pred))
    r2 = r2_score(y_val, y_pred)
    print(f"ðŸ“ˆ è©•ä¾¡æŒ‡æ¨™ (æ™‚é–“å¸¯åˆ¥ãƒ¢ãƒ‡ãƒ«): RMSE: ç´„ {rmse:.2f} äºº/30åˆ†, R2ã‚¹ã‚³ã‚¢: {r2:.3f}")

    # --- 4g. SHAPã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã®å¯è¦–åŒ– ---
    print("\n--- SHAPã§ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆã‚’å¯è¦–åŒ–ã—ã¾ã™ ---")
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_val)
    shap.summary_plot(shap_values, X_val, plot_type="bar", max_display=15, show=False)
    plt.title('SHAP Feature Importance (Time-Series Model)')
    plt.show()

    # --- 4h. æˆæžœç‰©ã®ä¿å­˜ ---
    model.save_model('model_A_timeseries.json')
    with open('columns_A_timeseries.json', 'w') as f:
        json.dump(list(X_train.columns), f)

    print("\nâœ… Aç—…é™¢ç”¨ã®æ™‚é–“å¸¯äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¨ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
    files.download('model_A_timeseries.json')
    files.download('columns_A_timeseries.json')

except Exception as e:
    print(f"\näºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


# ==== Cell 2 ====
# ðŸ¥ Aç—…é™¢ å¾…ã¡äººæ•°ãƒ»å¾…ã¡æ™‚é–“ åŒæ™‚äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (æœ€çµ‚ä¿®æ­£ç‰ˆ)
# =================================================================
# 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# =================================================================
!pip -q install japanize-matplotlib jpholiday scikit-learn xgboost shap

# =================================================================
# 2. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# =================================================================
import pandas as pd
import numpy as np
import jpholiday
from datetime import date, timedelta
import io
import json
import shap
import xgboost as xgb
import matplotlib.pyplot as plt
import japanize_matplotlib

from sklearn.metrics import mean_squared_error, r2_score
from google.colab import files
import warnings
warnings.filterwarnings('ignore')

# =================================================================
# 3. Aç—…é™¢ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# =================================================================
print("--- ðŸ¥ Aç—…é™¢ã®å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ ---")
print("1. æŽ¡è¡€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ« (ã€Œå—ä¿¡ã€ã€ŒæŒ‡ç¤ºæ›¸ã€ã€Œå‡¦ç†ã€åˆ—ã‚’å«ã‚€)")
uploaded_log = files.upload()
print("\n2. å¤–æ¥æ‚£è€…æ•°ãƒ•ã‚¡ã‚¤ãƒ«")
uploaded_patients = files.upload()
print("\n3. æ°—è±¡ãƒ‡ãƒ¼ã‚¿")
uploaded_weather = files.upload()

# =================================================================
# 4. ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# =================================================================
try:
    # --- 4a. å…¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨æº–å‚™ ---
    print("\n--- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åˆæœŸå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ ---")
    log_filename = list(uploaded_log.keys())[0]
    patients_filename = list(uploaded_patients.keys())[0]
    weather_filename = list(uploaded_weather.keys())[0]

    log_df = pd.read_csv(io.BytesIO(uploaded_log[log_filename]), encoding='shift-jis')
    total_patients_df = pd.read_csv(io.BytesIO(uploaded_patients[patients_filename]), encoding='utf-8-sig', thousands=',')
    weather_raw_df = pd.read_csv(io.BytesIO(uploaded_weather[weather_filename]), encoding='shift-jis', header=None, skiprows=3)

    log_df = log_df[log_df['å‡¦ç†'] == 'çµ‚äº†'].copy()
    print(f"âœ… ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œçµ‚äº†ã€ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã—ãŸã€‚å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(log_df)}")

    # --- 4b. å¾…ã¡æ™‚é–“ã¨å„ã‚¤ãƒ™ãƒ³ãƒˆæ™‚é–“ã®ç®—å‡º ---
    log_df['reception_time'] = pd.to_datetime(log_df['å—ä¿¡'], errors='coerce')
    log_df['call_time'] = pd.to_datetime(log_df['æŒ‡ç¤ºæ›¸'], errors='coerce')
    log_df.dropna(subset=['reception_time', 'call_time'], inplace=True)

    log_df['wait_minutes'] = (log_df['call_time'] - log_df['reception_time']).dt.total_seconds() / 60
    log_df = log_df[(log_df['wait_minutes'] >= 0) & (log_df['wait_minutes'] <= 180)]

    # --- 4c. 30åˆ†å˜ä½ã§ã®ãƒ‡ãƒ¼ã‚¿é›†è¨ˆ ---
    print("\n--- ãƒ‡ãƒ¼ã‚¿ã‚’30åˆ†å˜ä½ã«é›†è¨ˆã—ã¾ã™ ---")
    avg_wait = log_df.set_index('call_time')['wait_minutes'].resample('30T').mean().to_frame(name='avg_wait_minutes')
    receptions = log_df.set_index('reception_time').resample('30T').size().to_frame(name='reception_count')
    calls = log_df.set_index('call_time').resample('30T').size().to_frame(name='call_count')

    df_30min = pd.concat([avg_wait, receptions, calls], axis=1).fillna(0)
    df_30min = df_30min.between_time('08:00', '18:00')

    # --- 4d. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ---
    print("\n--- äºˆæ¸¬ç”¨ã®ç‰¹å¾´é‡ã‚’ä½œæˆã—ã¾ã™ ---")

    # ã‚¹ãƒ†ãƒƒãƒ— 1: ãƒžãƒ¼ã‚¸ã®å‰ã«ã€DatetimeIndexã‹ã‚‰æ™‚é–“ç‰¹å¾´é‡ã‚’å…ˆã«ä½œæˆ
    df_30min['hour'] = df_30min.index.hour
    df_30min['minute'] = df_30min.index.minute
    df_30min['dayofweek'] = df_30min.index.dayofweek
    df_30min['date'] = pd.to_datetime(df_30min.index.date)

    # ã‚¹ãƒ†ãƒƒãƒ— 2: æ—¥æ¬¡ç‰¹å¾´é‡ã®æº–å‚™
    daily_dates = pd.DataFrame({'date': pd.to_datetime(df_30min['date'].unique())})
    daily_dates['is_holiday'] = daily_dates['date'].apply(
        lambda x: jpholiday.is_holiday(x) or x.weekday() >= 5 or \
                    (x.month == 12 and x.day >= 29) or (x.month == 1 and x.day <= 3)
    )
    daily_dates['æœˆ'] = daily_dates['date'].dt.month
    daily_dates['é€±å›žæ•°'] = (daily_dates['date'].dt.day - 1) // 7 + 1
    daily_dates['å‰æ—¥ç¥æ—¥ãƒ•ãƒ©ã‚°'] = daily_dates['is_holiday'].shift(1).fillna(False).astype(int)

    total_patients_df['date'] = pd.to_datetime(total_patients_df['date'], errors='coerce')
    header_w = weather_raw_df.iloc[0]
    weather_df = weather_raw_df.iloc[3:].reset_index(drop=True); weather_df.columns = header_w
    weather_features = weather_df.iloc[:, [0, 1, 5, 8, 11, 14, 20, 23]].copy()
    weather_features.columns = ['date', 'é™æ°´é‡', 'å¤©æ°—æ¦‚æ³', 'å¹³å‡æ°—æ¸©', 'æœ€é«˜æ°—æ¸©', 'æœ€ä½Žæ°—æ¸©', 'å¹³å‡æ¹¿åº¦', 'å¹³å‡é¢¨é€Ÿ']
    weather_features['date'] = pd.to_datetime(weather_features['date'], errors='coerce')
    numeric_cols = ['é™æ°´é‡', 'å¹³å‡æ°—æ¸©', 'æœ€é«˜æ°—æ¸©', 'æœ€ä½Žæ°—æ¸©', 'å¹³å‡æ¹¿åº¦', 'å¹³å‡é¢¨é€Ÿ']
    for col in numeric_cols:
        weather_features[col] = pd.to_numeric(weather_features[col], errors='coerce')
        weather_features[col].fillna(weather_features[col].mean(), inplace=True)
    weather_features['é›¨ãƒ•ãƒ©ã‚°'] = weather_features['å¤©æ°—æ¦‚æ³'].str.contains('é›¨', na=False).astype(int)
    weather_features['é›ªãƒ•ãƒ©ã‚°'] = weather_features['å¤©æ°—æ¦‚æ³'].str.contains('é›ª', na=False).astype(int)
    weather_features['å¤©æ°—ã‚«ãƒ†ã‚´ãƒª'] = weather_features['å¤©æ°—æ¦‚æ³'].str[0]

    # ã‚¹ãƒ†ãƒƒãƒ— 3: å…¨ã¦ã®æ—¥æ¬¡ç‰¹å¾´é‡ã‚’ãƒžãƒ¼ã‚¸ (ã“ã“ã§IndexãŒãƒªã‚»ãƒƒãƒˆã•ã‚Œã‚‹)
    df_30min = pd.merge(df_30min, daily_dates, on='date', how='left')
    df_30min = pd.merge(df_30min, total_patients_df, on='date', how='left')
    df_30min = pd.merge(df_30min, weather_features.drop(columns=['å¤©æ°—æ¦‚æ³']), on='date', how='left')
    df_30min = pd.get_dummies(df_30min, columns=['dayofweek', 'å¤©æ°—ã‚«ãƒ†ã‚´ãƒª'], drop_first=True)

    # ã‚¹ãƒ†ãƒƒãƒ— 4: ãƒžãƒ¼ã‚¸å¾Œã«å¾…ã¡äººæ•°é–¢é€£ã®ç‰¹å¾´é‡ã‚’ä½œæˆ
    df_30min['net_flow'] = df_30min['reception_count'] - df_30min['call_count']
    df_30min['queue_size'] = df_30min.groupby('date')['net_flow'].cumsum()
    df_30min['queue_at_start_of_slot'] = df_30min['queue_size'].shift(1).fillna(0)
    day_starts = df_30min['date'] != df_30min['date'].shift(1)
    df_30min.loc[day_starts, 'queue_at_start_of_slot'] = 0

    df_30min.fillna(0, inplace=True)
    df_30min['is_holiday'] = df_30min['is_holiday'].astype(int)

    # --- 4e. å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åˆ†å‰² ---
    train_df = df_30min[df_30min['date'].dt.year == 2024].copy()
    val_df = df_30min[df_30min['date'].dt.year == 2025].copy()

    targets = ['avg_wait_minutes', 'queue_size']
    features = [col for col in train_df.columns if col not in targets + ['call_count', 'net_flow', 'date']]

    X_train, y_train = train_df[features], train_df[targets]
    X_val, y_val = val_df[features], val_df[targets]
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ä»¶, æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val)}ä»¶")

    # --- 4f. 2ã¤ã®XGBoostãƒ¢ãƒ‡ãƒ«ã‚’ãã‚Œãžã‚Œå­¦ç¿’ ---
    print("\n--- 2ã¤ã®äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«å­¦ç¿’ã—ã¾ã™ ---")

    model_waittime = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_estimators=500, learning_rate=0.05, max_depth=5)
    model_waittime.fit(X_train, y_train['avg_wait_minutes'])
    print("âœ… å¹³å‡å¾…ã¡æ™‚é–“äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    model_queue = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_estimators=500, learning_rate=0.05, max_depth=5)
    model_queue.fit(X_train, y_train['queue_size'])
    print("âœ… å¾…ã¡äººæ•°äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # --- 4g. ä¸¡ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ ---
    print("\n--- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ä¸¡ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã™ ---")
    pred_waittime = model_waittime.predict(X_val)
    rmse_wt = np.sqrt(mean_squared_error(y_val['avg_wait_minutes'], pred_waittime))
    r2_wt = r2_score(y_val['avg_wait_minutes'], pred_waittime)
    print(f"ðŸ“ˆ å¾…ã¡æ™‚é–“ãƒ¢ãƒ‡ãƒ«è©•ä¾¡: RMSE: ç´„ {rmse_wt:.2f} åˆ†, R2ã‚¹ã‚³ã‚¢: {r2_wt:.3f}")

    pred_queue = model_queue.predict(X_val)
    rmse_q = np.sqrt(mean_squared_error(y_val['queue_size'], pred_queue))
    r2_q = r2_score(y_val['queue_size'], pred_queue)
    print(f"ðŸ“ˆ å¾…ã¡äººæ•°ãƒ¢ãƒ‡ãƒ«è©•ä¾¡: RMSE: ç´„ {rmse_q:.2f} äºº, R2ã‚¹ã‚³ã‚¢: {r2_q:.3f}")

    # --- 4h. æˆæžœç‰©ã®ä¿å­˜ ---
    model_waittime.save_model('model_A_waittime_30min.json')
    model_queue.save_model('model_A_queue_30min.json')
    with open('columns_A_multi_30min.json', 'w') as f:
        json.dump(features, f)

    print("\nâœ… Aç—…é™¢ç”¨ã®å¾…ã¡æ™‚é–“ãƒ»å¾…ã¡äººæ•°äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«(30åˆ†å˜ä½)ã¨ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
    files.download('model_A_waittime_30min.json')
    files.download('model_A_queue_30min.json')
    files.download('columns_A_multi_30min.json')

except Exception as e:
    print(f"\näºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

