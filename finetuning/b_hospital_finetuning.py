"""
Bç—…é™¢ ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Automatically converted from Jupyter Notebook for reproducibility.
"""

# ==== Cell 0 ====
# ðŸ¥ Bç—…é™¢ æ—¥æ¬¡äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

# =================================================================
# 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# =================================================================
!pip -q install japanize-matplotlib jpholiday scikit-learn xgboost shap

# =================================================================
# 2. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# =================================================================
import io
import json
import warnings
from datetime import date, timedelta

import jpholiday
import joblib
import numpy as np
import pandas as pd
import xgboost as xgb
from google.colab import files
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings('ignore')

# =================================================================
# 3. ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# =================================================================
print("--- ðŸ¥ Bç—…é™¢ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™ ---")

print("\n--- Aç—…é™¢ã®å­¦ç¿’æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ ---")
print("1. Aç—…é™¢ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (model_A_daily.json)")
uploaded_model_A = files.upload()

print("\n2. Aç—…é™¢ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« (best_params_A_daily.json)")
uploaded_params_A = files.upload()

print("\n--- Bç—…é™¢ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ ---")
print("3. Bç—…é™¢ã®æŽ¡è¡€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«")
uploaded_log_B = files.upload()

print("\n4. Bç—…é™¢ã®å¤–æ¥æ‚£è€…æ•°ãƒ•ã‚¡ã‚¤ãƒ«")
uploaded_patients_B = files.upload()

print("\n5. æ°—è±¡ãƒ‡ãƒ¼ã‚¿")
uploaded_weather_B = files.upload()

# =================================================================
# 4. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
# =================================================================
try:
    # --- 4a. Bç—…é™¢ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç† ---
    print("\n--- Bç—…é™¢ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ ---")

    # ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ•ã‚£ãƒ¼ãƒãƒ£ä½œæˆ
    start_dt, end_dt = date(2024, 1, 1), date(2026, 12, 31)
    all_dates_list = [start_dt + timedelta(days=d) for d in range((end_dt - start_dt).days + 1)]
    calendar_features = pd.DataFrame({'date': pd.to_datetime(all_dates_list)})

    # ç¥æ—¥ãƒ»ä¼‘æ—¥åˆ¤å®š
    is_holiday_series = calendar_features['date'].apply(
        lambda x: (
            jpholiday.is_holiday(x)
            or x.weekday() >= 5           # åœŸæ—¥
            or (x.month == 12 and x.day >= 29)  # å¹´æœ«
            or (x.month == 1 and x.day <= 3)    # å¹´å§‹
        )
    )
    calendar_features['is_holiday'] = is_holiday_series
    calendar_features['æœˆ'] = calendar_features['date'].dt.month

    weekday_map = {0: 'æœˆ', 1: 'ç«', 2: 'æ°´', 3: 'æœ¨', 4: 'é‡‘', 5: 'åœŸ', 6: 'æ—¥'}
    calendar_features['æ›œæ—¥'] = calendar_features['date'].dt.dayofweek.map(weekday_map)
    calendar_features['é€±å›žæ•°'] = (calendar_features['date'].dt.day - 1) // 7 + 1
    calendar_features['å‰æ—¥ç¥æ—¥ãƒ•ãƒ©ã‚°'] = calendar_features['is_holiday'].shift(1).fillna(False)

    # æŽ¡è¡€ãƒ­ã‚°èª­ã¿è¾¼ã¿ï¼ˆBç—…é™¢ï¼‰
    log_df_b = pd.read_csv(io.BytesIO(next(iter(uploaded_log_B.values()))), encoding='shift-jis')
    log_df_b = log_df_b[log_df_b['å‡¦ç†'] == 'çµ‚äº†'].copy()
    print(f"âœ… ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œçµ‚äº†ã€ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã—ãŸã€‚å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(log_df_b)}")

    log_df_b['å®Ÿæ–½æ—¥'] = pd.to_datetime(log_df_b['å®Ÿæ–½æ—¥'], format='%y/%m/%d', errors='coerce')
    daily_blood_patients_b = log_df_b.groupby('å®Ÿæ–½æ—¥').size().reset_index(name='blood_patient_count')

    # å¤–æ¥æ‚£è€…æ•°ãƒ‡ãƒ¼ã‚¿ï¼ˆBç—…é™¢ï¼‰
    total_patients_df_b = pd.read_csv(
        io.BytesIO(next(iter(uploaded_patients_B.values()))),
        encoding='utf-8-sig',
        thousands=','
    )
    total_patients_df_b['date'] = pd.to_datetime(total_patients_df_b['date'], errors='coerce')

    # æ°—è±¡ãƒ‡ãƒ¼ã‚¿ï¼ˆBç—…é™¢ï¼šæ±äº¬ï¼‰
    weather_raw_df_b = pd.read_csv(
        io.BytesIO(next(iter(uploaded_weather_B.values()))),
        encoding='shift-jis',
        header=None,
        skiprows=3
    )

    # ãƒ˜ãƒƒãƒ€è¡Œå–å¾— & ãƒ‡ãƒ¼ã‚¿æœ¬ä½“
    header_b = weather_raw_df_b.iloc[0]
    weather_df_b = weather_raw_df_b.iloc[3:].reset_index(drop=True)
    weather_df_b.columns = header_b

    # å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿æŠ½å‡ºï¼ˆåˆ—ä½ç½®ã¯å…ƒãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã«ä¾å­˜ï¼‰
    df_selected_b = weather_df_b.iloc[:, [0, 1, 5, 8, 11, 14, 20, 23]].copy()
    df_selected_b.columns = ['date', 'é™æ°´é‡', 'å¤©æ°—æ¦‚æ³', 'å¹³å‡æ°—æ¸©', 'æœ€é«˜æ°—æ¸©', 'æœ€ä½Žæ°—æ¸©', 'å¹³å‡æ¹¿åº¦', 'å¹³å‡é¢¨é€Ÿ']

    # å¤©æ°—ãƒ•ãƒ©ã‚°
    df_selected_b['é›¨ãƒ•ãƒ©ã‚°'] = df_selected_b['å¤©æ°—æ¦‚æ³'].str.contains('é›¨', na=False).astype(int)
    df_selected_b['é›ªãƒ•ãƒ©ã‚°'] = df_selected_b['å¤©æ°—æ¦‚æ³'].str.contains('é›ª', na=False).astype(int)

    weather_features_b = df_selected_b.drop(columns=['å¤©æ°—æ¦‚æ³'])
    weather_features_b['date'] = pd.to_datetime(weather_features_b['date'], errors='coerce')

    # æ•°å€¤é …ç›®ã®æ¬ æè£œå®Œ
    numeric_cols = ['é™æ°´é‡', 'å¹³å‡æ°—æ¸©', 'æœ€é«˜æ°—æ¸©', 'æœ€ä½Žæ°—æ¸©', 'å¹³å‡æ¹¿åº¦', 'å¹³å‡é¢¨é€Ÿ']
    for col in numeric_cols:
        weather_features_b[col] = pd.to_numeric(weather_features_b[col], errors='coerce')
        weather_features_b[col].fillna(weather_features_b[col].mean(), inplace=True)

    # å„ãƒ‡ãƒ¼ã‚¿ã‚’ãƒžãƒ¼ã‚¸
    df_b = pd.merge(calendar_features, total_patients_df_b, on='date', how='left')
    df_b = pd.merge(df_b, daily_blood_patients_b, left_on='date', right_on='å®Ÿæ–½æ—¥', how='left')
    df_b = pd.merge(df_b, weather_features_b, on='date', how='left')

    # ç›®çš„å¤‰æ•°ãƒ»å¤–æ¥æ‚£è€…æ•°ãŒæƒã£ã¦ã„ã‚‹æ—¥ã®ã¿ä½¿ç”¨
    df_b.dropna(subset=['blood_patient_count', 'total_outpatient_count'], inplace=True)

    # --- 4b. æ›œæ—¥ãƒ€ãƒŸãƒ¼ã‚’ Aç—…é™¢ãƒ¢ãƒ‡ãƒ«ã¨æƒãˆã¦ä½œæˆ ---
    df_b_base = df_b.drop(columns=['is_holiday', 'å®Ÿæ–½æ—¥'], errors='ignore').copy()

    # æ›œæ—¥ãƒ€ãƒŸãƒ¼ã‚’å…¨ã¦ä½œæˆï¼ˆdrop_first=Falseï¼‰
    df_b_encoded = pd.get_dummies(df_b_base, columns=['æ›œæ—¥'], drop_first=False)

    # Aç—…é™¢ãƒ¢ãƒ‡ãƒ«ãŒä½¿ã£ã¦ã„ã‚‹æ›œæ—¥ãƒ€ãƒŸãƒ¼ï¼ˆå¹³æ—¥ã®ã¿ï¼‰
    weekday_keep = ['æ›œæ—¥_æœˆ', 'æ›œæ—¥_ç«', 'æ›œæ—¥_æ°´', 'æ›œæ—¥_æœ¨', 'æ›œæ—¥_é‡‘']

    # åœŸæ—¥ãŒã‚ã‚Œã°å‰Šé™¤
    for col in ['æ›œæ—¥_åœŸ', 'æ›œæ—¥_æ—¥']:
        if col in df_b_encoded.columns:
            df_b_encoded.drop(columns=[col], inplace=True)

    # è¶³ã‚Šãªã„æ›œæ—¥ãƒ€ãƒŸãƒ¼ãŒã‚ã‚Œã° 0 ã§è¿½åŠ 
    for col in weekday_keep:
        if col not in df_b_encoded.columns:
            df_b_encoded[col] = 0

    # ã€Œå‰æ—¥ç¥æ—¥ãƒ•ãƒ©ã‚°ã€ã¯ int ã«
    df_b_encoded['å‰æ—¥ç¥æ—¥ãƒ•ãƒ©ã‚°'] = df_b_encoded['å‰æ—¥ç¥æ—¥ãƒ•ãƒ©ã‚°'].astype(int)

    # Aç—…é™¢ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã—ã¦ã„ã‚‹ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆã‚’æ˜Žç¤ºçš„ã«æŒ‡å®š
    expected_features = [
        'æœˆ',
        'é€±å›žæ•°',
        'å‰æ—¥ç¥æ—¥ãƒ•ãƒ©ã‚°',
        'total_outpatient_count',
        'é™æ°´é‡',
        'å¹³å‡æ°—æ¸©',
        'æœ€é«˜æ°—æ¸©',
        'æœ€ä½Žæ°—æ¸©',
        'å¹³å‡æ¹¿åº¦',
        'å¹³å‡é¢¨é€Ÿ',
        'é›¨ãƒ•ãƒ©ã‚°',
        'é›ªãƒ•ãƒ©ã‚°',
        'æ›œæ—¥_æœˆ',
        'æ›œæ—¥_æœ¨',  # â˜…Aç—…é™¢ãƒ¢ãƒ‡ãƒ«å´ã®é †ç•ªãƒ»åå‰ã«åˆã‚ã›ã‚‹
        'æ›œæ—¥_æ°´',
        'æ›œæ—¥_ç«',
        'æ›œæ—¥_é‡‘'
    ]

    # è¶³ã‚Šãªã„åˆ—ãŒã‚ã‚Œã° 0 ã§è¿½åŠ 
    for col in expected_features:
        if col not in df_b_encoded.columns:
            df_b_encoded[col] = 0

    # --- 4c. Bç—…é™¢ã®ãƒ‡ãƒ¼ã‚¿åˆ†å‰² ---
    train_df_b = df_b_encoded[df_b_encoded['date'].dt.year == 2024].copy()
    val_df_b = df_b_encoded[(df_b_encoded['date'].dt.year == 2025) & (df_b_encoded['date'].dt.month <= 8)].copy()

    # ç‰¹å¾´é‡ã¯ expected_features ã ã‘ã‚’ä½¿ç”¨
    features_b = expected_features

    X_train_B, y_train_B = train_df_b[features_b], train_df_b['blood_patient_count']
    X_val_B, y_val_B = val_df_b[features_b], val_df_b['blood_patient_count']

    print(f"âœ… Bç—…é™¢ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†ã€‚å­¦ç¿’: {len(X_train_B)}ä»¶, æ¤œè¨¼: {len(X_val_B)}ä»¶")

    # --- 4d. XGBoost ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ---
    print("\n--- Aç—…é™¢ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™ ---")

    # Aç—…é™¢ãƒ¢ãƒ‡ãƒ«ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    model_a_filename = list(uploaded_model_A.keys())[0]
    model_a_bytes = uploaded_model_A[model_a_filename]
    temp_model_path = 'temp_model_A_for_finetuning.json'
    with open(temp_model_path, 'wb') as f:
        f.write(model_a_bytes)

    # Aç—…é™¢ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    best_params_from_A = json.load(io.BytesIO(next(iter(uploaded_params_A.values()))))
    print(f"\nAç—…é™¢ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªå‹•ã§èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {best_params_from_A}")

    # èª­ã¿è¾¼ã‚“ã ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    finetuned_model = xgb.XGBRegressor(
        objective='reg:squarederror',
        random_state=42,
        **best_params_from_A
    )

    # Aç—…é™¢ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã« Bç—…é™¢ãƒ‡ãƒ¼ã‚¿ã§è¿½åŠ å­¦ç¿’
    finetuned_model.fit(
        X_train_B,
        y_train_B,
        xgb_model=temp_model_path
    )
    print("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # --- 4e. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®æ€§èƒ½è©•ä¾¡ ---
    print("\n--- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã™ ---")
    y_pred_b = finetuned_model.predict(X_val_B)
    rmse_b = np.sqrt(mean_squared_error(y_val_B, y_pred_b))
    r2_b = r2_score(y_val_B, y_pred_b)
    print(f"ðŸ“ˆ è©•ä¾¡æŒ‡æ¨™ (Bç—…é™¢ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ): RMSE={rmse_b:.2f}, R2ã‚¹ã‚³ã‚¢={r2_b:.3f}")

    # --- 4f. Bç—…é™¢ç”¨ artifact ã®ä¿å­˜ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ---
    scaler_B = StandardScaler().fit(X_train_B)
    joblib.dump(scaler_B, 'model_B_daily_scaler.joblib')
    finetuned_model.save_model('model_B_daily_finetuned.json')

    print("\nâœ… Bç—…é™¢ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’2ã¤ä½œæˆã—ã¾ã—ãŸã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    files.download('model_B_daily_finetuned.json')
    files.download('model_B_daily_scaler.joblib')

except Exception as e:
    print(f"\näºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


# ==== Cell 1 ====
# ðŸ¥ Bç—…é™¢ æ™‚é–“å¸¯åˆ¥ãƒ¢ãƒ‡ãƒ« ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# =================================================================
# 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# =================================================================
!pip -q install japanize-matplotlib jpholiday scikit-learn xgboost shap

# =================================================================
# 2. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# =================================================================
import pandas as pd
import numpy as np
import jpholiday
from datetime import date, timedelta
import io
import json
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score
from google.colab import files
import warnings
warnings.filterwarnings('ignore')

# =================================================================
# 3. ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# =================================================================
print("--- ðŸ¥ Bç—…é™¢ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™ ---")
print("\n--- Aç—…é™¢ã®å­¦ç¿’æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ ---")
print("1. Aç—…é™¢ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (model_A_timeseries.json)")
uploaded_model_A = files.upload()
print("\n2. Aç—…é™¢ã®ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ (columns_A_timeseries.json)")
uploaded_columns_A = files.upload()

print("\n--- Bç—…é™¢ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ ---")
print("3. Bç—…é™¢ã®æŽ¡è¡€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«")
uploaded_log_B = files.upload()
print("\n4. Bç—…é™¢ã®å¤–æ¥æ‚£è€…æ•°ãƒ•ã‚¡ã‚¤ãƒ«")
uploaded_patients_B = files.upload()
print("\n5. æ°—è±¡ãƒ‡ãƒ¼ã‚¿")
uploaded_weather_B = files.upload()

# =================================================================
# 4. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
# =================================================================
try:
    # --- 4a. å…¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
    print("\n--- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åˆæœŸå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ ---")
    log_df = pd.read_csv(io.BytesIO(next(iter(uploaded_log_B.values()))), encoding='shift-jis')
    total_patients_df = pd.read_csv(io.BytesIO(next(iter(uploaded_patients_B.values()))), encoding='utf-8-sig', thousands=',')
    weather_raw_df = pd.read_csv(io.BytesIO(next(iter(uploaded_weather_B.values()))), encoding='shift-jis', header=None, skiprows=3)
    columns_A = json.load(io.BytesIO(next(iter(uploaded_columns_A.values()))))

    # ã€Œå‡¦ç†ã€åˆ—ãŒã€Œçµ‚äº†ã€ã®è¡Œã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    log_df = log_df[log_df['å‡¦ç†'] == 'çµ‚äº†'].copy()
    print(f"âœ… ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œçµ‚äº†ã€ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã—ãŸã€‚å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(log_df)}")

    # --- 4b. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ ---
    print("\n--- ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’30åˆ†å˜ä½ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã—ã¾ã™ ---")
    log_df['time'] = pd.to_datetime(log_df['å—ä¿¡'], errors='coerce')
    log_df.dropna(subset=['time'], inplace=True)
    log_df.set_index('time', inplace=True)
    patient_count_by_slot = log_df.resample('30T').size().rename('patient_count_slot')
    patient_count_by_slot = patient_count_by_slot.between_time('08:00', '18:00')
    df_ts = patient_count_by_slot.to_frame()

    # --- 4c. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ---
    print("\n--- æ™‚é–“å¸¯äºˆæ¸¬ç”¨ã®ç‰¹å¾´é‡ã‚’ä½œæˆã—ã¾ã™ ---")
    df_ts['hour'] = df_ts.index.hour
    df_ts['minute'] = df_ts.index.minute
    df_ts['dayofweek'] = df_ts.index.dayofweek
    df_ts['date'] = pd.to_datetime(df_ts.index.date)
    df_ts['is_first_slot'] = ((df_ts['hour'] == 8) & (df_ts['minute'] == 0)).astype(int)
    df_ts['is_second_slot'] = ((df_ts['hour'] == 8) & (df_ts['minute'] == 30)).astype(int)
    daily_dates = pd.DataFrame({'date': pd.to_datetime(df_ts['date'].unique())})
    daily_dates['is_holiday_daily'] = daily_dates['date'].apply(
        lambda x: jpholiday.is_holiday(x) or x.weekday() >= 5 or \
                    (x.month == 12 and x.day >= 29) or (x.month == 1 and x.day <= 3)
    )
    daily_dates['æœˆ'] = daily_dates['date'].dt.month
    daily_dates['é€±å›žæ•°'] = (daily_dates['date'].dt.day - 1) // 7 + 1
    daily_dates['å‰æ—¥ç¥æ—¥ãƒ•ãƒ©ã‚°'] = daily_dates['is_holiday_daily'].shift(1).fillna(False).astype(int)
    total_patients_df['date'] = pd.to_datetime(total_patients_df['date'], errors='coerce')
    header_w = weather_raw_df.iloc[0]
    weather_df = weather_raw_df.iloc[3:].reset_index(drop=True); weather_df.columns = header_w
    weather_features = weather_df.iloc[:, [0, 1, 5, 8, 11, 14, 20, 23]].copy()
    weather_features.columns = ['date', 'é™æ°´é‡', 'å¤©æ°—æ¦‚æ³', 'å¹³å‡æ°—æ¸©', 'æœ€é«˜æ°—æ¸©', 'æœ€ä½Žæ°—æ¸©', 'å¹³å‡æ¹¿åº¦', 'å¹³å‡é¢¨é€Ÿ']
    weather_features['date'] = pd.to_datetime(weather_features['date'], errors='coerce')
    numeric_cols = ['é™æ°´é‡', 'å¹³å‡æ°—æ¸©', 'æœ€é«˜æ°—æ¸©', 'æœ€ä½Žæ°—æ¸©', 'å¹³å‡æ¹¿åº¦', 'å¹³å‡é¢¨é€Ÿ']
    for col in numeric_cols:
        weather_features[col] = pd.to_numeric(weather_features[col], errors='coerce')
        weather_features[col].fillna(weather_features[col].mean(), inplace=True)
    weather_features['é›¨ãƒ•ãƒ©ã‚°'] = weather_features['å¤©æ°—æ¦‚æ³'].str.contains('é›¨', na=False).astype(int)
    weather_features['é›ªãƒ•ãƒ©ã‚°'] = weather_features['å¤©æ°—æ¦‚æ³'].str.contains('é›ª', na=False).astype(int)
    weather_features['å¤©æ°—ã‚«ãƒ†ã‚´ãƒª'] = weather_features['å¤©æ°—æ¦‚æ³'].str[0]
    df_ts = pd.merge(df_ts, daily_dates, on='date', how='left')
    df_ts = pd.merge(df_ts, total_patients_df, on='date', how='left')
    df_ts = pd.merge(df_ts, weather_features.drop(columns=['å¤©æ°—æ¦‚æ³']), on='date', how='left')
    df_ts = pd.get_dummies(df_ts, columns=['dayofweek', 'å¤©æ°—ã‚«ãƒ†ã‚´ãƒª'], drop_first=True)
    df_ts['lag_30min'] = df_ts['patient_count_slot'].shift(1)
    df_ts['lag_60min'] = df_ts['patient_count_slot'].shift(2)
    df_ts['lag_90min'] = df_ts['patient_count_slot'].shift(3)
    df_ts['rolling_mean_60min'] = df_ts['patient_count_slot'].shift(1).rolling(window=2).mean()
    df_ts.rename(columns={'is_holiday_daily': 'is_holiday'}, inplace=True)
    df_ts.fillna(0, inplace=True)

    # Aç—…é™¢ã®å­¦ç¿’æ™‚ã¨ç‰¹å¾´é‡ã®æ§‹æˆã‚’å®Œå…¨ã«ä¸€è‡´ã•ã›ã‚‹
    for col in columns_A:
        if col not in df_ts.columns:
            df_ts[col] = 0 # Bç—…é™¢ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ãªã„ç‰¹å¾´é‡ãŒã‚ã‚Œã°0ã§åŸ‹ã‚ã‚‹
    df_ts = df_ts[columns_A + ['date', 'patient_count_slot']] # ã‚«ãƒ©ãƒ ã®é †åºã‚’Aç—…é™¢ã¨åˆã‚ã›ã‚‹

    # --- 4d. Bç—…é™¢ã®ãƒ‡ãƒ¼ã‚¿åˆ†å‰² ---
    train_df = df_ts[df_ts['date'].dt.year == 2024].copy()
    val_df = df_ts[(df_ts['date'].dt.year == 2025) & (df_ts['date'].dt.month <= 8)].copy()
    train_df.drop(columns=['date'], inplace=True)
    val_df.drop(columns=['date'], inplace=True)

    features = [col for col in train_df.columns if col != 'patient_count_slot']
    X_train_B, y_train_B = train_df[features], train_df['patient_count_slot']
    X_val_B, y_val_B = val_df[features], val_df['patient_count_slot']
    print(f"\nâœ… Bç—…é™¢ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†ã€‚å­¦ç¿’: {len(X_train_B)}ä»¶, æ¤œè¨¼: {len(X_val_B)}ä»¶")

    # --- 4e. XGBoostãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ---
    print("\n--- Aç—…é™¢ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™ ---")

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸAç—…é™¢ãƒ¢ãƒ‡ãƒ«ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    model_a_filename = list(uploaded_model_A.keys())[0]
    model_a_bytes = uploaded_model_A[model_a_filename]
    temp_model_path = 'temp_model_A_for_finetuning.json'
    with open(temp_model_path, 'wb') as f:
        f.write(model_a_bytes)

    # Aç—…é™¢ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ å­¦ç¿’
    finetuned_model = xgb.XGBRegressor(
        objective='reg:squarederror', n_estimators=1000, learning_rate=0.01,
        max_depth=6, early_stopping_rounds=50, random_state=42
    )
    finetuned_model.fit(
        X_train_B, y_train_B,
        eval_set=[(X_val_B, y_val_B)],
        xgb_model=temp_model_path,
        verbose=False
    )
    print("âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # --- 4f. æ€§èƒ½è©•ä¾¡ ---
    print("\n--- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã™ ---")
    y_pred_b = finetuned_model.predict(X_val_B)
    rmse_b = np.sqrt(mean_squared_error(y_val_B, y_pred_b))
    r2_b = r2_score(y_val_B, y_pred_b)
    print(f"ðŸ“ˆ è©•ä¾¡æŒ‡æ¨™ (Bç—…é™¢ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ): RMSE={rmse_b:.2f} äºº/30åˆ†, R2ã‚¹ã‚³ã‚¢={r2_b:.3f}")

    # --- 4g. æˆæžœç‰©ã®ä¿å­˜ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ---
    finetuned_model.save_model('model_B_timeseries_finetuned.json')
    with open('columns_B_timeseries.json', 'w') as f:
        json.dump(features, f)

    print("\nâœ… Bç—…é™¢ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’2ã¤ä½œæˆã—ã¾ã—ãŸã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    files.download('model_B_timeseries_finetuned.json')
    files.download('columns_B_timeseries.json')

except Exception as e:
    print(f"\näºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


# ==== Cell 2 ====
# ðŸ¥ Bç—…é™¢ å¾…ã¡äººæ•°ãƒ»å¾…ã¡æ™‚é–“ãƒ¢ãƒ‡ãƒ« ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
# =================================================================
# 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# =================================================================
!pip -q install japanize-matplotlib jpholiday scikit-learn xgboost shap

# =================================================================
# 2. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# =================================================================
import pandas as pd
import numpy as np
import jpholiday
from datetime import date, timedelta
import io
import json
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score
from google.colab import files
import warnings
warnings.filterwarnings('ignore')

# =================================================================
# 3. ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# =================================================================
print("--- ðŸ¥ Bç—…é™¢ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™ ---")
print("\n--- Aç—…é™¢ã®å­¦ç¿’æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ ---")
print("1. Aç—…é™¢ã®å¾…ã¡æ™‚é–“ãƒ¢ãƒ‡ãƒ« (model_A_waittime_30min.json)")
uploaded_model_waittime_A = files.upload()
print("\n2. Aç—…é™¢ã®å¾…ã¡äººæ•°ãƒ¢ãƒ‡ãƒ« (model_A_queue_30min.json)")
uploaded_model_queue_A = files.upload()
print("\n3. Aç—…é™¢ã®ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ (columns_A_multi_30min.json)")
uploaded_columns_A = files.upload()

print("\n--- Bç—…é™¢ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ ---")
print("4. Bç—…é™¢ã®æŽ¡è¡€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«")
uploaded_log_B = files.upload()
print("\n5. Bç—…é™¢ã®å¤–æ¥æ‚£è€…æ•°ãƒ•ã‚¡ã‚¤ãƒ«")
uploaded_patients_B = files.upload()
print("\n6. æ°—è±¡ãƒ‡ãƒ¼ã‚¿")
uploaded_weather_B = files.upload()

# =================================================================
# 4. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
# =================================================================
try:
    # --- 4a. å…¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
    print("\n--- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åˆæœŸå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ ---")
    log_df = pd.read_csv(io.BytesIO(next(iter(uploaded_log_B.values()))), encoding='shift-jis')
    total_patients_df = pd.read_csv(io.BytesIO(next(iter(uploaded_patients_B.values()))), encoding='utf-8-sig', thousands=',')
    weather_raw_df = pd.read_csv(io.BytesIO(next(iter(uploaded_weather_B.values()))), encoding='shift-jis', header=None, skiprows=3)
    columns_A = json.load(io.BytesIO(next(iter(uploaded_columns_A.values()))))

    log_df = log_df[log_df['å‡¦ç†'] == 'çµ‚äº†'].copy()
    print(f"âœ… ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œçµ‚äº†ã€ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã—ãŸã€‚å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(log_df)}")

    # --- 4b. å¾…ã¡æ™‚é–“ã¨å„ã‚¤ãƒ™ãƒ³ãƒˆæ™‚é–“ã®ç®—å‡º ---
    log_df['reception_time'] = pd.to_datetime(log_df['å—ä¿¡'], errors='coerce')
    log_df['call_time'] = pd.to_datetime(log_df['æŒ‡ç¤ºæ›¸'], errors='coerce')
    log_df.dropna(subset=['reception_time', 'call_time'], inplace=True)
    log_df['wait_minutes'] = (log_df['call_time'] - log_df['reception_time']).dt.total_seconds() / 60
    log_df = log_df[(log_df['wait_minutes'] >= 0) & (log_df['wait_minutes'] <= 180)]

    # --- 4c. 30åˆ†å˜ä½ã§ã®ãƒ‡ãƒ¼ã‚¿é›†è¨ˆ ---
    print("\n--- ãƒ‡ãƒ¼ã‚¿ã‚’30åˆ†å˜ä½ã«é›†è¨ˆã—ã¾ã™ ---")
    avg_wait = log_df.set_index('call_time')['wait_minutes'].resample('30T').mean().to_frame(name='avg_wait_minutes')
    receptions = log_df.set_index('reception_time').resample('30T').size().to_frame(name='reception_count')
    calls = log_df.set_index('call_time').resample('30T').size().to_frame(name='call_count')
    df_30min = pd.concat([avg_wait, receptions, calls], axis=1).fillna(0)
    df_30min = df_30min.between_time('08:00', '18:00')

    # --- 4d. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ---
    print("\n--- äºˆæ¸¬ç”¨ã®ç‰¹å¾´é‡ã‚’ä½œæˆã—ã¾ã™ ---")
    df_30min['hour'] = df_30min.index.hour
    df_30min['minute'] = df_30min.index.minute
    df_30min['dayofweek'] = df_30min.index.dayofweek
    df_30min['date'] = pd.to_datetime(df_30min.index.date)

    daily_dates = pd.DataFrame({'date': pd.to_datetime(df_30min['date'].unique())})
    daily_dates['is_holiday'] = daily_dates['date'].apply(
        lambda x: jpholiday.is_holiday(x) or x.weekday() >= 5 or \
                    (x.month == 12 and x.day >= 29) or (x.month == 1 and x.day <= 3)
    )
    daily_dates['æœˆ'] = daily_dates['date'].dt.month
    daily_dates['é€±å›žæ•°'] = (daily_dates['date'].dt.day - 1) // 7 + 1
    daily_dates['å‰æ—¥ç¥æ—¥ãƒ•ãƒ©ã‚°'] = daily_dates['is_holiday'].shift(1).fillna(False).astype(int)

    total_patients_df['date'] = pd.to_datetime(total_patients_df['date'], errors='coerce')
    header_w = weather_raw_df.iloc[0]
    weather_df = weather_raw_df.iloc[3:].reset_index(drop=True); weather_df.columns = header_w
    weather_features = weather_df.iloc[:, [0, 1, 5, 8, 11, 14, 20, 23]].copy()
    weather_features.columns = ['date', 'é™æ°´é‡', 'å¤©æ°—æ¦‚æ³', 'å¹³å‡æ°—æ¸©', 'æœ€é«˜æ°—æ¸©', 'æœ€ä½Žæ°—æ¸©', 'å¹³å‡æ¹¿åº¦', 'å¹³å‡é¢¨é€Ÿ']
    weather_features['date'] = pd.to_datetime(weather_features['date'], errors='coerce')
    numeric_cols = ['é™æ°´é‡', 'å¹³å‡æ°—æ¸©', 'æœ€é«˜æ°—æ¸©', 'æœ€ä½Žæ°—æ¸©', 'å¹³å‡æ¹¿åº¦', 'å¹³å‡é¢¨é€Ÿ']
    for col in numeric_cols:
        weather_features[col] = pd.to_numeric(weather_features[col], errors='coerce')
        weather_features[col].fillna(weather_features[col].mean(), inplace=True)
    weather_features['é›¨ãƒ•ãƒ©ã‚°'] = weather_features['å¤©æ°—æ¦‚æ³'].str.contains('é›¨', na=False).astype(int)
    weather_features['é›ªãƒ•ãƒ©ã‚°'] = weather_features['å¤©æ°—æ¦‚æ³'].str.contains('é›ª', na=False).astype(int)
    weather_features['å¤©æ°—ã‚«ãƒ†ã‚´ãƒª'] = weather_features['å¤©æ°—æ¦‚æ³'].str[0]

    df_30min = pd.merge(df_30min, daily_dates, on='date', how='left')
    df_30min = pd.merge(df_30min, total_patients_df, on='date', how='left')
    df_30min = pd.merge(df_30min, weather_features.drop(columns=['å¤©æ°—æ¦‚æ³']), on='date', how='left')
    df_30min = pd.get_dummies(df_30min, columns=['dayofweek', 'å¤©æ°—ã‚«ãƒ†ã‚´ãƒª'], drop_first=True)

    df_30min['net_flow'] = df_30min['reception_count'] - df_30min['call_count']
    df_30min['queue_size'] = df_30min.groupby('date')['net_flow'].cumsum()
    df_30min['queue_at_start_of_slot'] = df_30min['queue_size'].shift(1).fillna(0)
    day_starts = df_30min['date'] != df_30min['date'].shift(1)
    df_30min.loc[day_starts, 'queue_at_start_of_slot'] = 0

    df_30min.fillna(0, inplace=True)
    df_30min['is_holiday'] = df_30min['is_holiday'].astype(int)

    # Aç—…é™¢ã®å­¦ç¿’æ™‚ã¨ç‰¹å¾´é‡ã®æ§‹æˆã‚’å®Œå…¨ã«ä¸€è‡´ã•ã›ã‚‹
    for col in columns_A:
        if col not in df_30min.columns:
            df_30min[col] = 0
    df_30min = df_30min[columns_A + ['date', 'avg_wait_minutes', 'queue_size']]

    # --- 4e. Bç—…é™¢ã®ãƒ‡ãƒ¼ã‚¿åˆ†å‰² ---
    train_df = df_30min[df_30min['date'].dt.year == 2024].copy()
    val_df = df_30min[df_30min['date'].dt.year == 2025].copy()

    targets = ['avg_wait_minutes', 'queue_size']
    features = [col for col in train_df.columns if col not in targets + ['date']]

    X_train_B, y_train_B = train_df[features], train_df[targets]
    X_val_B, y_val_B = val_df[features], val_df[targets]
    print(f"âœ… Bç—…é™¢ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†ã€‚å­¦ç¿’: {len(X_train_B)}ä»¶, æ¤œè¨¼: {len(X_val_B)}ä»¶")

    # --- 4f. 2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãã‚Œãžã‚Œãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° ---
    print("\n--- 2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ ---")

    # Aç—…é™¢ãƒ¢ãƒ‡ãƒ«ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    model_waittime_A_filename = list(uploaded_model_waittime_A.keys())[0]
    with open('temp_model_waittime_A.json', 'wb') as f:
        f.write(uploaded_model_waittime_A[model_waittime_A_filename])

    model_queue_A_filename = list(uploaded_model_queue_A.keys())[0]
    with open('temp_model_queue_A.json', 'wb') as f:
        f.write(uploaded_model_queue_A[model_queue_A_filename])

    # ãƒ¢ãƒ‡ãƒ«1: å¾…ã¡æ™‚é–“ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    model_waittime_ft = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
    model_waittime_ft.fit(X_train_B, y_train_B['avg_wait_minutes'], xgb_model='temp_model_waittime_A.json')
    print("âœ… å¹³å‡å¾…ã¡æ™‚é–“äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # ãƒ¢ãƒ‡ãƒ«2: å¾…ã¡äººæ•°ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    model_queue_ft = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
    model_queue_ft.fit(X_train_B, y_train_B['queue_size'], xgb_model='temp_model_queue_A.json')
    print("âœ… å¾…ã¡äººæ•°äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # --- 4g. ä¸¡ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ ---
    print("\n--- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ä¸¡ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã™ ---")
    pred_waittime = model_waittime_ft.predict(X_val_B)
    rmse_wt = np.sqrt(mean_squared_error(y_val_B['avg_wait_minutes'], pred_waittime))
    r2_wt = r2_score(y_val_B['avg_wait_minutes'], pred_waittime)
    print(f"ðŸ“ˆ å¾…ã¡æ™‚é–“ãƒ¢ãƒ‡ãƒ«è©•ä¾¡: RMSE: ç´„ {rmse_wt:.2f} åˆ†, R2ã‚¹ã‚³ã‚¢: {r2_wt:.3f}")

    pred_queue = model_queue_ft.predict(X_val_B)
    rmse_q = np.sqrt(mean_squared_error(y_val_B['queue_size'], pred_queue))
    r2_q = r2_score(y_val_B['queue_size'], pred_queue)
    print(f"ðŸ“ˆ å¾…ã¡äººæ•°ãƒ¢ãƒ‡ãƒ«è©•ä¾¡: RMSE: ç´„ {rmse_q:.2f} äºº, R2ã‚¹ã‚³ã‚¢: {r2_q:.3f}")

    # --- 4h. æˆæžœç‰©ã®ä¿å­˜ ---
    model_waittime_ft.save_model('model_B_waittime_30min_ft.json')
    model_queue_ft.save_model('model_B_queue_30min_ft.json')
    with open('columns_B_multi_30min.json', 'w') as f:
        json.dump(features, f)

    print("\nâœ… Bç—…é™¢ç”¨ã®å¾…ã¡æ™‚é–“ãƒ»å¾…ã¡äººæ•°äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«(30åˆ†å˜ä½)ã¨ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
    files.download('model_B_waittime_30min_ft.json')
    files.download('model_B_queue_30min_ft.json')
    files.download('columns_B_multi_30min.json')

except Exception as e:
    print(f"\näºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

